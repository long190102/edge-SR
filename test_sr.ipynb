{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from models.srcnn import *\n",
    "from models.vdsr import *\n",
    "\n",
    "from models.sr_model import *\n",
    "from models.hqsr import *\n",
    "from models.vdsr import *\n",
    "from models.srresnet import *\n",
    "from models.sr_model import *\n",
    "from models.vdsr import *\n",
    "from models.utils import *\n",
    "from models.srcnn import *\n",
    "from models.edsr import *\n",
    "import time\n",
    "from models.e2dsr import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "t = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "def calculate_metrics(img1, img2, max_pixel_value=1.0):\n",
    "    psnr = PeakSignalNoiseRatio(data_range=1.0).to(device)\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "    psnr_value = psnr(img1, img2)\n",
    "    ssim_value = ssim(img1, img2)\n",
    "    return psnr_value, ssim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, type =False, scale = 4):\n",
    "        self.lr_files = sorted(os.listdir(lr_dir))\n",
    "        self.hr_files = sorted(os.listdir(hr_dir))\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.type =type\n",
    "        self.scale = scale\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_image = Image.open(os.path.join(self.lr_dir, self.lr_files[idx])).convert('RGB')\n",
    "        hr_image = Image.open(os.path.join(self.hr_dir, self.hr_files[idx])).convert('RGB')\n",
    "        width, height = hr_image.size\n",
    "        lr_height, lr_width = height // self.scale, width // self.scale\n",
    "        hr_height, hr_width = lr_height * self.scale, lr_width*self.scale\n",
    "        lr_image = lr_image.resize((lr_width, lr_height))\n",
    "        hr_image = hr_image.resize((hr_width, hr_height))\n",
    "        if self.type:\n",
    "            lr_image = lr_image.resize((hr_width, hr_height))\n",
    "            \n",
    "        # hr_image = cv2.resize(hr_image, (hr_width, hr_height), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            # transforms.ToPILImage(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        lr_image = transform(lr_image)\n",
    "        hr_image = transform(hr_image)\n",
    "        return lr_image, hr_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2023919/2275431609.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  edsr.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_edsr.pth', map_location=device))\n",
      "/tmp/ipykernel_2023919/2275431609.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  srresnet.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_srresnet.pth', map_location=device))\n",
      "/tmp/ipykernel_2023919/2275431609.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vdsr.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_vdsr.pth', map_location=device))\n",
      "/tmp/ipykernel_2023919/2275431609.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  srcnn.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_srcnn.pth', map_location=device))\n",
      "/tmp/ipykernel_2023919/2275431609.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  e2dsr_canny.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_e2dsr_canny.pth', map_location=device))\n",
      "/tmp/ipykernel_2023919/2275431609.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  e2dsr_sobel.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_e2dsr_sobel.pth', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 2\n",
      "BSDS100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?batch/s]/home/robot/anaconda3/envs/pcb/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `PeakSignalNoiseRatio` from `torchmetrics` was deprecated and will be removed in 2.0. Import `PeakSignalNoiseRatio` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "/home/robot/anaconda3/envs/pcb/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `StructuralSimilarityIndexMeasure` from `torchmetrics` was deprecated and will be removed in 2.0. Import `StructuralSimilarityIndexMeasure` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n",
      "100%|██████████| 100/100 [00:03<00:00, 31.09batch/s]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.07batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIV2K\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.02batch/s]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.53s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 34.01batch/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 11.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 20.63batch/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  6.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  6.87batch/s]\n",
      "100%|██████████| 100/100 [00:44<00:00,  2.24batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 3\n",
      "BSDS100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 30.56batch/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIV2K\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.99batch/s]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 36.14batch/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 18.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 20.75batch/s]\n",
      "100%|██████████| 14/14 [00:01<00:00, 10.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.48batch/s]\n",
      "100%|██████████| 100/100 [00:30<00:00,  3.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale 4\n",
      "BSDS100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 31.49batch/s]\n",
      "100%|██████████| 100/100 [00:05<00:00, 18.41batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIV2K\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.02batch/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 38.50batch/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 22.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 22.28batch/s]\n",
      "100%|██████████| 14/14 [00:01<00:00, 12.77batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  7.06batch/s]\n",
      "100%|██████████| 100/100 [00:23<00:00,  4.32batch/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "        # sub = 'Set14'\n",
    "for scale in[2, 3, 4]:\n",
    "    # edsr_srcnn = EDSR_srcnnfy()\n",
    "    # edsr  = EDSR_orig().to(device)\n",
    "    # scale = 4\n",
    "    print(f'scale {scale}')\n",
    "    log_file = open('outputs/test_log/subtracttion.txt', 'a')\n",
    "    log_file.write(f'scale {scale}')\n",
    "    edsr = EDSR(scale).to(device)\n",
    "    edrn_canny = HQSR(scale_factor=scale, use_canny=True).to(device)\n",
    "    edrn_sobel = HQSR(scale_factor=scale,use_sobel=True).to(device)\n",
    "    srresnet = SRResNet(scale=scale).to(device)\n",
    "    vdsr = VDSR().to(device)\n",
    "    e2dsr_sobel = E2DSR(scale_factor=scale, edge_option='sobel').to(device)\n",
    "    e2dsr_canny = E2DSR(scale_factor=scale,edge_option='canny').to(device)\n",
    "    srcnn = SRCNN().to(device)\n",
    "\n",
    "    edsr.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_edsr.pth', map_location=device))\n",
    "    # edrn_sobel.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_hqsr_sobel.pth', map_location=device))\n",
    "    # edrn_canny.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_hqsr_canny.pth', map_location=device))\n",
    "    srresnet.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_srresnet.pth', map_location=device))\n",
    "    vdsr.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_vdsr.pth', map_location=device))\n",
    "    srcnn.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_srcnn.pth', map_location=device))\n",
    "    e2dsr_canny.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_e2dsr_canny.pth', map_location=device))\n",
    "    e2dsr_sobel.load_state_dict(torch.load(f'outputs/weight_sr/x{scale}/best_e2dsr_sobel.pth', map_location=device))\n",
    "    for sub in ['BSDS100', 'DIV2K', 'Set5', 'Set14', 'Urban100']:\n",
    "        print(f'{sub}\\n')\n",
    "        valid_lr_dir = f'benchmark/{sub}/HR'\n",
    "        valid_hr_dir = f'benchmark/{sub}/HR'\n",
    "        output_image_dir = f'benchmark/output/{sub}'\n",
    "        valid_dataset = ImageDataset(valid_lr_dir, valid_hr_dir,scale=scale)\n",
    "        valid_dataset_2 = ImageDataset(valid_lr_dir, valid_hr_dir, type = True, scale=scale)\n",
    "        valid_loader = DataLoader(valid_dataset)\n",
    "        valid_loader_2 = DataLoader(valid_dataset_2)\n",
    "        log_file.write(f'{sub}\\n')\n",
    "        # Đo thời gian xử lý trung bình cho từng mô hình\n",
    "        e2dsr_sobel_total_time = 0\n",
    "        e2dsr_canny_total_time = 0\n",
    "        sobel_total_time = 0\n",
    "        canny_total_time = 0\n",
    "        edsr_total_time = 0\n",
    "        srresnet_total_time = 0\n",
    "        vdsr_total_time = 0\n",
    "        srcnn_total_time = 0\n",
    "        bicubic_total_time = 0\n",
    "\n",
    "        edrn_sobel.eval()\n",
    "        edrn_canny.eval()\n",
    "        srresnet.eval()\n",
    "        srcnn.eval()\n",
    "        edsr.eval()\n",
    "        vdsr.eval()\n",
    "        e2dsr_canny.eval()\n",
    "        e2dsr_sobel.eval()\n",
    "\n",
    "        val_psnr_values_bicubic = 0\n",
    "        val_psnr_values_sobel = 0\n",
    "        val_psnr_values_canny = 0\n",
    "        val_psnr_values_edsr = 0\n",
    "        val_psnr_values_srresnet = 0\n",
    "        val_psnr_values_vdsr = 0\n",
    "        val_psnr_values_srcnn = 0\n",
    "        val_psnr_values_e2dsr_canny = 0\n",
    "        val_psnr_values_e2dsr_sobel = 0\n",
    "\n",
    "        val_ssim_values_sobel = 0\n",
    "        val_ssim_values_canny = 0\n",
    "        val_ssim_values_edsr = 0\n",
    "        val_ssim_values_srresnet = 0\n",
    "        val_ssim_values_vdsr = 0\n",
    "        val_ssim_values_srcnn = 0\n",
    "        val_ssim_values_bicubic = 0\n",
    "        val_ssim_values_e2dsr_canny = 0\n",
    "        val_ssim_values_e2dsr_sobel = 0\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():  # Không tính toán gradient trong quá trình validation\n",
    "            for (lr_images, hr_images) in tqdm(valid_loader_2, unit='batch'):\n",
    "                lr_images = lr_images.to(device)\n",
    "                hr_images = hr_images.to(device)\n",
    "                # VDSR validation\n",
    "                start_time = time.time()\n",
    "                outputs_vdsr = vdsr(lr_images)\n",
    "                vdsr_total_time += time.time() - start_time\n",
    "                psnr_vdsr, ssim_vdsr = calculate_metrics(outputs_vdsr, hr_images)\n",
    "\n",
    "                # SRCNN validation\n",
    "                start_time = time.time()\n",
    "                outputs_srcnn = srcnn(lr_images)\n",
    "                srcnn_total_time += time.time() - start_time\n",
    "                psnr_srcnn, ssim_srcnn = calculate_metrics(outputs_srcnn, hr_images)\n",
    "\n",
    "                # Cập nhật PSNR và SSIM cho VDSR và SRCNN\n",
    "                val_psnr_values_vdsr += psnr_vdsr\n",
    "                val_psnr_values_srcnn += psnr_srcnn\n",
    "\n",
    "                val_ssim_values_vdsr += ssim_vdsr\n",
    "                val_ssim_values_srcnn += ssim_srcnn\n",
    "\n",
    "            # Tính PSNR và SSIM trung bình cho VDSR và SRCNN\n",
    "            val_average_psnr_vdsr = val_psnr_values_vdsr / len(valid_loader_2)\n",
    "            val_average_psnr_srcnn = val_psnr_values_srcnn / len(valid_loader_2)\n",
    "\n",
    "            val_average_ssim_vdsr = val_ssim_values_vdsr / len(valid_loader_2)\n",
    "            val_average_ssim_srcnn = val_ssim_values_srcnn / len(valid_loader_2)\n",
    "\n",
    "            # Thời gian xử lý trung bình cho VDSR và SRCNN\n",
    "            avg_time_vdsr = vdsr_total_time / len(valid_loader_2)\n",
    "            avg_time_srcnn = srcnn_total_time / len(valid_loader_2)\n",
    "\n",
    "            # Ghi kết quả vào log_file\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():  # Không tính toán gradient trong quá trình validation\n",
    "            for (lr_images, hr_images) in tqdm(valid_loader, unit='batch'):\n",
    "                lr_images = lr_images.to(device)\n",
    "                hr_images = hr_images.to(device)\n",
    "\n",
    "                # Sobel SR validation (không tính loss, chỉ tính PSNR và SSIM)\n",
    "                start_time = time.time()  # Đo thời gian bắt đầu\n",
    "                outputs_sobel = edrn_sobel(lr_images)\n",
    "                sobel_total_time += time.time() - start_time  # Tính thời gian xử lý\n",
    "                psnr_sobel, ssim_sobel = calculate_metrics(outputs_sobel, hr_images)\n",
    "\n",
    "                #bicubic\n",
    "                start_time = time.time()  # Đo thời gian bắt đầu\n",
    "                outputs_bicubic = F.interpolate(lr_images, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "                bicubic_total_time += time.time() - start_time  # Tính thời gian xử lý bicubic\n",
    "                psnr_bicubic, ssim_bicubic = calculate_metrics(outputs_bicubic, hr_images)\n",
    "\n",
    "                # Canny SR validation\n",
    "                start_time = time.time()\n",
    "                outputs_canny = edrn_canny(lr_images)\n",
    "                canny_total_time += time.time() - start_time\n",
    "                psnr_canny, ssim_canny = calculate_metrics(outputs_canny, hr_images)\n",
    "\n",
    "                # Canny SR validation\n",
    "                start_time = time.time()\n",
    "                outputs_e2dsr_canny = e2dsr_canny(lr_images)\n",
    "                e2dsr_canny_total_time += time.time() - start_time\n",
    "                psnr_e2dsr_canny, ssim_e2dsr_canny = calculate_metrics(outputs_e2dsr_canny, hr_images)\n",
    "\n",
    "                # Canny SR validation\n",
    "                start_time = time.time()\n",
    "                outputs_e2dsr_sobel = e2dsr_sobel(lr_images)\n",
    "                e2dsr_sobel_total_time += time.time() - start_time\n",
    "                psnr_e2dsr_sobel, ssim_e2dsr_sobel = calculate_metrics(outputs_e2dsr_sobel, hr_images)\n",
    "\n",
    "                # EDSR SR validation\n",
    "                start_time = time.time()\n",
    "                outputs_edsr = edsr(lr_images)\n",
    "                edsr_total_time += time.time() - start_time\n",
    "                psnr_edsr, ssim_edsr = calculate_metrics(outputs_edsr, hr_images)\n",
    "\n",
    "                # SRResNet SR validation\n",
    "                start_time = time.time()\n",
    "                outputs_srresnet = srresnet(lr_images)\n",
    "                srresnet_total_time += time.time() - start_time\n",
    "                psnr_srresnet, ssim_srresnet = calculate_metrics(outputs_srresnet, hr_images)\n",
    "\n",
    "                # Cập nhật PSNR và SSIM\n",
    "                val_psnr_values_sobel += psnr_sobel\n",
    "                val_psnr_values_canny += psnr_canny\n",
    "                val_psnr_values_edsr += psnr_edsr\n",
    "                val_psnr_values_srresnet += psnr_srresnet\n",
    "                val_psnr_values_bicubic += psnr_bicubic\n",
    "                val_psnr_values_e2dsr_canny += psnr_e2dsr_canny\n",
    "                val_psnr_values_e2dsr_sobel += psnr_e2dsr_sobel\n",
    "\n",
    "                val_ssim_values_sobel += ssim_sobel\n",
    "                val_ssim_values_canny += ssim_canny\n",
    "                val_ssim_values_edsr += ssim_edsr\n",
    "                val_ssim_values_srresnet += ssim_srresnet\n",
    "                val_ssim_values_bicubic += ssim_bicubic\n",
    "                val_ssim_values_e2dsr_canny += ssim_e2dsr_canny\n",
    "                val_ssim_values_e2dsr_sobel += ssim_e2dsr_sobel\n",
    "            # Tính PSNR và SSIM trung bình\n",
    "            val_average_psnr_sobel = val_psnr_values_sobel / len(valid_loader)\n",
    "            val_average_psnr_canny = val_psnr_values_canny / len(valid_loader)\n",
    "            val_average_psnr_edsr = val_psnr_values_edsr / len(valid_loader)\n",
    "            val_average_psnr_srresnet = val_psnr_values_srresnet / len(valid_loader)\n",
    "            val_average_psnr_bicubic = val_psnr_values_bicubic / len(valid_loader)\n",
    "            val_average_psnr_e2dsr_canny = val_psnr_values_e2dsr_canny/len(valid_loader)\n",
    "            val_average_psnr_e2dsr_sobel  = val_psnr_values_e2dsr_sobel / len(valid_loader)\n",
    "\n",
    "            \n",
    "            val_average_ssim_sobel = val_ssim_values_sobel / len(valid_loader)\n",
    "            val_average_ssim_canny = val_ssim_values_canny / len(valid_loader)\n",
    "            val_average_ssim_edsr = val_ssim_values_edsr / len(valid_loader)\n",
    "            val_average_ssim_srresnet = val_ssim_values_srresnet / len(valid_loader)\n",
    "            val_average_ssim_bicubic = val_ssim_values_bicubic / len(valid_loader)\n",
    "            val_average_ssim_e2dsr_canny = val_ssim_values_e2dsr_canny/ len(valid_loader)\n",
    "            val_average_ssim_e2dsr_sobel = val_ssim_values_e2dsr_sobel/len(valid_loader)\n",
    "            # Thời gian xử lý trung bình\n",
    "            avg_time_sobel = sobel_total_time / len(valid_loader)\n",
    "            avg_time_canny = canny_total_time / len(valid_loader)\n",
    "            avg_time_edsr = edsr_total_time / len(valid_loader)\n",
    "            avg_time_srresnet = srresnet_total_time / len(valid_loader)\n",
    "            avg_time_bicubic = bicubic_total_time / len(valid_loader)\n",
    "            avg_time_e2dsr_canny = e2dsr_canny_total_time/len(valid_loader)\n",
    "            avg_time_e2dsr_sobel = e2dsr_sobel_total_time/len(valid_loader)\n",
    "\n",
    "            # Thời gian xử lý trung bình cho Bicubic\n",
    "            \n",
    "            temp_psnr = val_average_psnr_bicubic\n",
    "            temp_ssim = val_average_ssim_bicubic\n",
    "            # Ghi kết quả vào log_file\n",
    "            # Validation cho VDSR và SRCNN\n",
    "        log_file.write(f'Bicubic:  SSIM / PSNR {val_average_ssim_bicubic:.4f} / {val_average_psnr_bicubic:.2f}, Time {avg_time_bicubic:.4f}s\\n')\n",
    "        log_file.write(f'VDSR:  SSIM / PSNR {val_average_ssim_vdsr:.4f} / {val_average_psnr_vdsr:.2f}, Time {avg_time_vdsr:.4f}s\\n')\n",
    "        log_file.write(f'SRCNN:  SSIM / PSNR {val_average_ssim_srcnn:.4f} / {val_average_psnr_srcnn:.2f}, Time {avg_time_srcnn:.4f}s\\n')    \n",
    "        log_file.write(f'SRResNet:  SSIM / PSNR {val_average_ssim_srresnet:.4f} / {val_average_psnr_srresnet:.2f}, Time {avg_time_srresnet:.4f}s\\n')\n",
    "        log_file.write(f'EDSR:  SSIM / PSNR {val_average_ssim_edsr:.4f} / {val_average_psnr_edsr:.2f}, Time {avg_time_edsr:.4f}s\\n')\n",
    "        log_file.write(f'Sobel:  SSIM / PSNR {val_average_ssim_sobel:.4f} / {val_average_psnr_sobel:.2f}, Time {avg_time_sobel:.4f}s\\n')\n",
    "        log_file.write(f'Canny:  SSIM / PSNR {val_average_ssim_canny:.4f} / {val_average_psnr_canny:.2f}, Time {avg_time_canny:.4f}s\\n')\n",
    "        log_file.write(f'E2DSR Sobel:  SSIM / PSNR {val_average_ssim_e2dsr_sobel:.4f} / {val_average_psnr_e2dsr_sobel:.2f}, Time {avg_time_e2dsr_sobel:.4f}s\\n')\n",
    "        log_file.write(f'E2DSR Canny:  SSIM / PSNR {val_average_ssim_e2dsr_canny:.4f} / {val_average_psnr_e2dsr_canny:.2f}, Time {avg_time_e2dsr_canny:.4f}s\\n\\n')\n",
    "        log_file.flush()\n",
    "        log_file.close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import time\n",
    "# from PIL import ImageDraw\n",
    "# transform = transforms.ToTensor()\n",
    "\n",
    "# sub = 'Urban100'\n",
    "# box = 100\n",
    "# hr_image_dir = f'benchmark/{sub}/HR'\n",
    "# lr_image_dir = hr_image_dir\n",
    "# output_image_dir = f'benchmark/output/{sub}'\n",
    "# os.makedirs(output_image_dir, exist_ok=True)\n",
    "# # lr_image_dir = valid_lr_dir\n",
    "# # hr_image_dir = valid_hr_dir\n",
    "# # index = random.randint(0, len(os.listdir(lr_image_dir))-1)\n",
    "# file = 'img_062.png'\n",
    "# lr_image_file = file\n",
    "# hr_image_file = file\n",
    "# with torch.no_grad():\n",
    "#     # for (lr_image_file, hr_image_file) in zip(sorted(os.listdir(lr_image_dir)), sorted(os.listdir(hr_image_dir))):\n",
    "#         # Đường dẫn đến ảnh\n",
    "\n",
    "#         lr_image_path = os.path.join(lr_image_dir, lr_image_file)\n",
    "#         hr_image_path = os.path.join(hr_image_dir, hr_image_file)\n",
    "#         ####################################\n",
    "#         img = cv2.imread(hr_image_path)  # Thay bằng đường dẫn ảnh của bạn\n",
    "#         cv2.namedWindow(\"Image\")\n",
    "#         cv2.imshow(\"Image\", img)\n",
    "#         drawing = False  # True khi đang nhấn chuột\n",
    "#         top_left_corner = [0,0]\n",
    "#         xy = [0, 0, 0, 0]\n",
    "#         def draw_rectangle(event, x, y, flags, param):\n",
    "#             global drawing, top_left_corner, img, xy\n",
    "            \n",
    "#             # Khi nhấn chuột trái, bắt đầu vẽ\n",
    "#             if event == cv2.EVENT_LBUTTONDOWN:\n",
    "#                 drawing = True\n",
    "#                 top_left_corner = [x, y]   # Lưu tọa độ của điểm bắt đầu\n",
    "            \n",
    "#             # Khi thả chuột trái, kết thúc vẽ\n",
    "#             elif event == cv2.EVENT_LBUTTONUP:\n",
    "#                 drawing = False\n",
    "                \n",
    "#                 xy[0] = top_left_corner[0]\n",
    "#                 xy[1] = top_left_corner[1]\n",
    "#                 xy[2] = top_left_corner[0] + box # type: ignore\n",
    "#                 xy[3] = top_left_corner[1] + box\n",
    "                \n",
    "#                 cv2.destroyAllWindows()\n",
    "#                 # Đóng cửa sổ ảnh sau khi vẽ xong\n",
    "#         # Đặt callback cho sự kiện chuột\n",
    "#         cv2.setMouseCallback(\"Image\", draw_rectangle)\n",
    "\n",
    "#         # Hiển thị ảnh và đợi cho đến khi bounding box được vẽ\n",
    "#         cv2.waitKey(0)\n",
    "#         print(xy)\n",
    "#         ####################################\n",
    "#         edsr_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_edsr.jpg')\n",
    "#         edrn_sobel_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_edrn_sobel.jpg')\n",
    "#         edrn_canny_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_edrn_canny.jpg')\n",
    "#         bicubic_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_bicubic.jpg')\n",
    "#         srresnet_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_srresnet.jpg')\n",
    "#         srcnn_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_srcnn.jpg')\n",
    "#         vdsr_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_vdsr.jpg')\n",
    "#         e2dsr_sobel_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_e2dsr_sobel.jpg')\n",
    "#         e2dsr_canny_path = os.path.join(output_image_dir, lr_image_file[:-4] + '_e2dsr_canny.jpg')\n",
    "        \n",
    "#         hr_path = os.path.join(output_image_dir, hr_image_file)\n",
    "#         # Tải và chuyển đổi ảnh\n",
    "#         lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "#         hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "#         w, h = hr_image.size\n",
    "#         lr_image = lr_image.resize((w//4, h//4))\n",
    "\n",
    "#         bicubic = lr_image.resize((w, h))\n",
    "#         lr_image_copy = lr_image.copy()\n",
    "#         hr_image_copy = hr_image.copy()\n",
    "#         # lr_image_1dms = torch.Tensor(lr_image_1dms)\n",
    "#         lr_image = transform(lr_image).unsqueeze(0).to(device)  # Thêm batch dimension và chuyển sang CPU\n",
    "#         hr_image = transform(hr_image).unsqueeze(0).to(device)  # Thêm batch dimension và chuyển sang CPU\n",
    "#         bicubic_ = transform(bicubic).unsqueeze(0).to(device)\n",
    "#         # print(type(lr_image_1dms))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # bicubic = lr_image_copy.resize((600, 600), resample=Image.BICUBIC) # type: ignore\n",
    "#         # bicubic_ = transform(bicubic).unsqueeze(0).to(device)\n",
    "#         # lr_image_1dms, _ = preprocess(lr_image_copy, device)\n",
    "#         # lr_image_copy, _ = preprocess(lr_image_copy, device)\n",
    "#         # hr_image_copy, _ = preprocess(HR, device)\n",
    "#         # _, ycbcr = preprocess(bicubic, device)\n",
    "#         time_pro = []\n",
    "#         def measure_inference_time(model, input_image, model_name):\n",
    "#             start_time = time.time()\n",
    "#             output = model(input_image)\n",
    "#             end_time = time.time()\n",
    "            \n",
    "#             inference_time = end_time - start_time\n",
    "#             time_pro.append(inference_time)\n",
    "#             return output\n",
    "        \n",
    "#         # Dự đoán\n",
    "#         output_edsr = edsr(lr_image)\n",
    "#         output_edrn_sobel = edrn_sobel(lr_image)\n",
    "#         output_edrn_canny = edrn_canny(lr_image)\n",
    "#         output_srresnet = srresnet(lr_image)\n",
    "#         output_srcnn = srcnn(bicubic_)\n",
    "#         output_vdsr = vdsr(bicubic_)\n",
    "#         output_e2dsr_sobel = e2dsr_sobel(lr_image)\n",
    "#         output_e2dsr_canny = e2dsr_canny(lr_image)        \n",
    "#         models = ['EDSR', 'EDRN Sobel', 'EDRN Canny', 'SRResNet', 'SRCNN', 'VDSR', 'E2DSR Canny', 'E2DSR Sobel']\n",
    "#         psnr_value = []  # Lưu giá trị PSNR cho mỗi mô hình\n",
    "#         ssim_value = []\n",
    "#         # Tính PSNR cho từng mô hình (giả sử output của các mô hình đã có)\n",
    "#         for model_output in [output_edsr, output_edrn_sobel, output_edrn_canny, output_srresnet, output_srcnn, output_vdsr, output_e2dsr_canny, output_e2dsr_sobel]:\n",
    "#             # Tính PSNR và thêm vào danh sách\n",
    "#             psnr_value.append(calculate_metrics(model_output, hr_image)[0])\n",
    "#             ssim_value.append(calculate_metrics(model_output, hr_image)[1])\n",
    "#         # Ghi PSNR cho từng mô hình vào file\n",
    "#         with open(f\"{output_image_dir}/results.txt\", \"a\") as psnr_file:\n",
    "#             psnr_file.write(f\"HR Image: {hr_image_file}\\n\")\n",
    "#             for i, model_name in enumerate(models):\n",
    "#                 psnr_file.write(f\"{model_name} PSNR/SSIM: {psnr_value[i]:.2f} dB/ {ssim_value[i]:.4f}\\n\")\n",
    "#             psnr_file.write(f\"Bicubic PSNR/SSIM: {calculate_metrics(bicubic_, hr_image)[0]:.2f} dB/ {calculate_metrics(bicubic_, hr_image)[1]:.4f}\\n\")\n",
    "#             psnr_file.write(\"\\n\")  # Dòng trống giữa các lần lặp\n",
    "            \n",
    "            \n",
    "#         output_image_edsr = output_edsr.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "#         output_image_edsr = transforms.ToPILImage()(output_image_edsr)  # Chuyển tensor thành ảnh PIL\n",
    "#         output_image_edsr = output_image_edsr.crop(xy)\n",
    "#         output_image_edsr.resize((100, 100))\n",
    "#         output_image_edsr.save(edsr_path)  # Lưu ảnh\n",
    "        \n",
    "#         output_image_edrn_sobel = output_edrn_sobel.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "#         output_image_edrn_sobel = transforms.ToPILImage()(output_image_edrn_sobel)  # Chuyển tensor thành ảnh PIL\n",
    "#         output_image_edrn_sobel = output_image_edrn_sobel.crop(xy)\n",
    "#         output_image_edrn_sobel.resize((100, 100))\n",
    "#         output_image_edrn_sobel.save(edrn_sobel_path)  # Lưu ảnh\n",
    "        \n",
    "        \n",
    "#         output_image_edrn_canny = output_edrn_canny.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "#         output_image_edrn_canny = transforms.ToPILImage()(output_image_edrn_canny)  # Chuyển tensor thành ảnh PIL\n",
    "#         output_image_edrn_canny = output_image_edrn_canny.crop(xy)\n",
    "#         output_image_edrn_canny.resize((100, 100))\n",
    "#         output_image_edrn_canny.save(edrn_canny_path)  # Lưu ảnh\n",
    "        \n",
    "#         output_image_srcnn = output_srcnn.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "#         output_image_srcnn = transforms.ToPILImage()(output_image_srcnn)  # Chuyển tensor thành ảnh PIL\n",
    "#         output_image_srcnn = output_image_srcnn.crop(xy)\n",
    "#         output_image_srcnn.resize((100, 100))\n",
    "#         output_image_srcnn.save(srcnn_path)  # Lưu ảnh\n",
    "        \n",
    "#         # output_vdsr = output_vdsr.mul(255.0).to(device).numpy().squeeze(0).squeeze(0)\n",
    "#         # output_image_vdsr = np.array([output_vdsr, ycbcr[..., 1], ycbcr[..., 2]]).transpose([1, 2, 0])\n",
    "#         # output_image_vdsr = np.clip(convert_ycbcr_to_rgb(output_image_vdsr ), 0.0, 255.0).astype(np.uint8)\n",
    "#         # output_image_vdsr = Image.fromarray(output_image_vdsr )\n",
    "#         # output_image_vdsr = output_image_vdsr.crop(xy)\n",
    "#         # output_image_vdsr .save(vdsr_path) \n",
    "        \n",
    "#         output_image_vdsr = output_vdsr.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "#         output_image_vdsr = transforms.ToPILImage()(output_image_vdsr)  # Chuyển tensor thành ảnh PIL\n",
    "#         output_image_vdsr = output_image_vdsr.crop(xy)\n",
    "#         output_image_vdsr.resize((100, 100))\n",
    "#         output_image_vdsr.save(vdsr_path)  # Lưu ản\n",
    "        \n",
    "#         output_image_srresnet = output_srresnet.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "#         output_image_srresnet = transforms.ToPILImage()(output_image_srresnet)  # Chuyển tensor thành ảnh PIL\n",
    "#         output_image_srresnet = output_image_srresnet.crop(xy)\n",
    "#         output_image_srresnet.resize((100, 100))\n",
    "#         output_image_srresnet.save(srresnet_path)  # Lưu ảnh\n",
    "        \n",
    "#         output_image_e2dsr_sobel = output_e2dsr_sobel.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "#         output_image_e2dsr_sobel = transforms.ToPILImage()(output_image_e2dsr_sobel)  # Chuyển tensor thành ảnh PIL\n",
    "#         output_image_e2dsr_sobel = output_image_e2dsr_sobel.crop(xy)\n",
    "#         output_image_e2dsr_sobel.resize((100, 100))\n",
    "#         output_image_e2dsr_sobel.save(e2dsr_sobel_path)  # Lưu ảnh\n",
    "        \n",
    "        \n",
    "#         output_image_e2dsr_canny = output_e2dsr_canny.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "#         output_image_e2dsr_canny = transforms.ToPILImage()(output_image_e2dsr_canny)  # Chuyển tensor thành ảnh PIL\n",
    "#         output_image_e2dsr_canny = output_image_e2dsr_canny.crop(xy)\n",
    "#         output_image_e2dsr_canny.resize((100, 100))\n",
    "#         output_image_e2dsr_canny.save(e2dsr_canny_path)  # Lưu ảnh\n",
    "#         # bicubic = lr_image.resize((600, 600))\n",
    "#         bicubic = bicubic.crop(xy)\n",
    "#         bicubic.resize((100, 100))\n",
    "#         bicubic.save(bicubic_path)\n",
    "        \n",
    "#         hr_image_crop = hr_image_copy.crop(xy)\n",
    "#         hr_image_crop.resize((100, 100))\n",
    "#         hr_image_crop.save(hr_path)\n",
    "        \n",
    "#         draw = ImageDraw.Draw(hr_image_copy)\n",
    "#         draw.rectangle(xy, outline=\"red\", width=3)\n",
    "#         hr_image_copy = hr_image_copy.resize((600, 600))\n",
    "#         hr_image_copy.save(f\"{output_image_dir}/{hr_image_file[:-4]}_with_bounding_box.jpg\")\n",
    "#         print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (sub, model) in zip(['HQSR_C', 'HQSR_S','E2DSR_C', 'E2DSR_S', 'EDSR', 'SRResNet', 'VDSR', 'SRCNN'], [edrn_canny, edrn_sobel, e2dsr_canny, e2dsr_sobel,edsr,  srresnet, vdsr, srcnn]):\n",
    "for (sub, model) in zip([ 'HQSR_C', 'HQSR_S'], [edrn_canny,edrn_sobel]):\n",
    "\n",
    "    # sub = 'EDSR'\n",
    "    # model = edsr\n",
    "    # lr_image_dir = f'sr_data/test/LR'\n",
    "\n",
    "    hr_image_dir = f'dataset/pcb/pcb_test/images'\n",
    "    lr_image_dir = hr_image_dir\n",
    "    output_image_dir = f'output/{sub}/images'\n",
    "    os.makedirs(output_image_dir, exist_ok = True)\n",
    "    # Duyệt qua các ảnh trong thư mục\n",
    "    lr_image_files = os.listdir(lr_image_dir)\n",
    "    hr_image_files = os.listdir(hr_image_dir)\n",
    "    lr_image_files.sort()\n",
    "    hr_image_files.sort()\n",
    "\n",
    "    psnr_dict = {\n",
    "        \"mouse_bite\" :[],\n",
    "        \"spur_\":[], \n",
    "        \"missing_hole\":[],\n",
    "        \"short\":[],\n",
    "        \"open_circuit\":[],\n",
    "        \"spurious_copper\":[]\n",
    "\n",
    "    }\n",
    "    ssim_dict = {\n",
    "        \"mouse_bite\" :[],\n",
    "        \"spur_\":[], \n",
    "        \"missing_hole\":[],\n",
    "        \"short\":[],\n",
    "        \"open_circuit\":[],\n",
    "        \"spurious_copper\":[]\n",
    "\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    transform = transforms.ToTensor()\n",
    "    with torch.no_grad():\n",
    "        for lr_image_file, hr_image_file in tqdm(zip(lr_image_files, hr_image_files), unit = 'img'):\n",
    "            # Đường dẫn đến ảnh\n",
    "            lr_image_path = os.path.join(lr_image_dir, lr_image_file)\n",
    "            hr_image_path = os.path.join(hr_image_dir, hr_image_file)\n",
    "            output_image_path = os.path.join(output_image_dir, hr_image_file)\n",
    "\n",
    "            # Tải và chuyển đổi ảnh\n",
    "            lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "            hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "            lr_image = hr_image.resize((150, 150))\n",
    "            if model == vdsr or model == srcnn:\n",
    "                lr_image = lr_image.resize((600, 600))\n",
    "            lr_image = transform(lr_image).unsqueeze(0).to(device)  # Thêm batch dimension và chuyển sang GPU\n",
    "            hr_image = transform(hr_image).unsqueeze(0).to(device)  # Thêm batch dimension và chuyển sang GPU\n",
    "\n",
    "            # Dự đoán\n",
    "            output = model(lr_image)\n",
    "\n",
    "            # Tính toán PSNR\n",
    "            psnr,ssim = calculate_metrics(output, hr_image)\n",
    "            for key in psnr_dict.keys():\n",
    "                if key in lr_image_path:\n",
    "                    psnr_dict[key].append(psnr)\n",
    "                    ssim_dict[key].append(ssim)\n",
    "                    break\n",
    "\n",
    "            # Chuyển đổi tensor đầu ra thành ảnh và lưu\n",
    "            output_image = output.squeeze(0).to(device)  # Loại bỏ batch dimension và chuyển tensor sang CPU\n",
    "            output_image = transforms.ToPILImage()(output_image)  # Chuyển tensor thành ảnh PIL\n",
    "            output_image.save(output_image_path)  # Lưu ảnh\n",
    "    avg_psnr = [0, 0, 0, 0, 0, 0]\n",
    "    avg_ssim = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "\n",
    "    # Tính toán PSNR trung bình\n",
    "    avg_psnr[0] = sum(psnr_dict['mouse_bite'])/len(psnr_dict['mouse_bite']) #mousebite_psnr\n",
    "    avg_psnr[1] = sum(psnr_dict['spur_'])/len(psnr_dict['spur_']) #spur_psnr\n",
    "    avg_psnr[2] = sum(psnr_dict['missing_hole'])/len(psnr_dict['missing_hole']) #missinghole_psnr \n",
    "    avg_psnr[3] = sum(psnr_dict['short'])/len(psnr_dict['short']) #short_psnr\n",
    "    avg_psnr[4] = sum(psnr_dict['open_circuit'])/len(psnr_dict['open_circuit']) #opencircuit_psnr\n",
    "    avg_psnr[5]= sum(psnr_dict['spurious_copper'])/len(psnr_dict['spurious_copper']) #spuriouscopper_psnr \n",
    "    average_psnr = sum(avg_psnr)/len(avg_psnr)\n",
    "\n",
    "    avg_ssim[0] = sum(ssim_dict['mouse_bite'])/len(ssim_dict['mouse_bite']) #mousebite_ssim\n",
    "    avg_ssim[1] = sum(ssim_dict['spur_'])/len(ssim_dict['spur_']) #spur_ssim\n",
    "    avg_ssim[2] = sum(ssim_dict['missing_hole'])/len(ssim_dict['missing_hole']) #missinghole_ssim \n",
    "    avg_ssim[3] = sum(ssim_dict['short'])/len(ssim_dict['short']) #short_ssim\n",
    "    avg_ssim[4] = sum(ssim_dict['open_circuit'])/len(ssim_dict['open_circuit']) #opencircuit_ssim\n",
    "    avg_ssim[5]= sum(ssim_dict['spurious_copper'])/len(ssim_dict['spurious_copper']) #spuriouscopper_ssim  \n",
    "    average_ssim = sum(avg_ssim)/len(avg_ssim)\n",
    "    end = time.time()\n",
    "\n",
    "    with open('results.txt', 'a') as f:\n",
    "        f.write(output_image_dir.split('/')[1] + '\\n')\n",
    "        f.write(f'missinghole_psnr: {avg_psnr[2]:.2f}' + '\\n')\n",
    "        f.write(f'mousebite_psnr: {avg_psnr[0]:.2f}' + '\\n')\n",
    "        f.write(f'opencircuit_psnr: {avg_psnr[4]:.2f}' + '\\n')\n",
    "        f.write(f'short_psnr: {avg_psnr[3]:.2f}' + '\\n')\n",
    "        f.write(f'spur_psnr: {avg_psnr[1]:.2f}' + '\\n')\n",
    "        f.write(f'spuriouscopper_psnr: {avg_psnr[5]:.2f}' + '\\n')\n",
    "        f.write(f'average_psnr: {average_psnr:.2f}' + '\\n')\n",
    "        f.write(f'time process: {end - start:.2f}' + '\\n')\n",
    "\n",
    "        f.write(f'missinghole_psnr: {avg_ssim[2]:.4f}' + '\\n')\n",
    "        f.write(f'mousebite_psnr: {avg_ssim[0]:.4f}' + '\\n')\n",
    "        f.write(f'opencircuit_psnr: {avg_ssim[4]:.4f}' + '\\n')\n",
    "        f.write(f'short_psnr: {avg_ssim[3]:.4f}' + '\\n')\n",
    "        f.write(f'spur_psnr: {avg_ssim[1]:.4f}' + '\\n')\n",
    "        f.write(f'spuriouscopper_psnr: {avg_ssim[5]:.4f}' + '\\n')\n",
    "        f.write(f'average_psnr: {average_ssim:.4f}' + '\\n')\n",
    "        f.write(f'time process: {end - start:.4f}' + '\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    # tạo dữ liệu cho YOLO\n",
    "    print(output_image_dir.split('/')[1] + '\\n')\n",
    "    print(f'missinghole_psnr: {avg_psnr[2]:.2f}' + '\\n')\n",
    "    print(f'mousebite_psnr: {avg_psnr[0]:.2f}' + '\\n')\n",
    "    print(f'opencircuit_psnr: {avg_psnr[4]:.2f}' + '\\n')\n",
    "    print(f'short_psnr: {avg_psnr[3]:.2f}' + '\\n')\n",
    "    print(f'spur_psnr: {avg_psnr[1]:.2f}' + '\\n')\n",
    "    print(f'spuriouscopper_psnr: {avg_psnr[5]:.2f}' + '\\n')\n",
    "    print(f'average_psnr: {average_psnr:.2f}' + '\\n')\n",
    "    print(f'time process: {end - start:.2f}' + '\\n')\n",
    "    print(f'missinghole_ssim: {avg_ssim[2]:.4f}' + '\\n')\n",
    "    print(f'mousebite_ssim: {avg_ssim[0]:.4f}' + '\\n')\n",
    "    print(f'opencircuit_ssim: {avg_ssim[4]:.4f}' + '\\n')\n",
    "    print(f'short_ssim: {avg_ssim[3]:.4f}' + '\\n')\n",
    "    print(f'spur_ssim: {avg_ssim[1]:.4f}' + '\\n')\n",
    "    print(f'spuriouscopper_ssim: {avg_ssim[5]:.4f}' + '\\n')\n",
    "    print(f'average_ssim: {average_ssim:.4f}' + '\\n')\n",
    "    print(f'time process: {end - start:.2f}' + '\\n')\n",
    "    # Đường dẫn đến thư mục nguồn và đích\n",
    "    source_dir = f'dataset/pcb/pcb_test/labels'\n",
    "\n",
    "    dest = f'output/{sub}/labels'\n",
    "\n",
    "    # Tạo thư mục đích nếu chưa tồn tại\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "\n",
    "    # Sao chép các file từ thư mục nguồn sang thư mục đích\n",
    "    for filename in os.listdir(source_dir):\n",
    "        source_file = os.path.join(source_dir, filename)\n",
    "        file1 = os.path.join(dest, filename)\n",
    "\n",
    "        shutil.copy(source_file, file1)\n",
    "\n",
    "    print(\"Đã sao chép các file thành công.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File config.yaml đã được tạo và ghi thành công.\n",
      "Ultralytics 8.3.9 🚀 Python-3.12.4 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 10971MiB)\n",
      "Model summary (fused): 168 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/robot/Desktop/pcb_temp/output/HQSR_C/labels... 1068 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1068/1068 [00:00<00:00, 2625.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/robot/Desktop/pcb_temp/output/HQSR_C/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1068/1068 [00:05<00:00, 191.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1068       2158      0.967      0.962      0.984      0.683\n",
      "            mouse_bite        168        332      0.964      0.979      0.991      0.688\n",
      "                  spur        169        348      0.911      0.948      0.978      0.665\n",
      "          missing_hole        190        379      0.989      0.988      0.994      0.718\n",
      "                 short        184        366      0.975      0.967      0.985      0.685\n",
      "          open_circuit        166        345       0.99      0.988      0.994      0.673\n",
      "       spurious_copper        191        388      0.972        0.9      0.961      0.672\n",
      "Speed: 0.5ms preprocess, 2.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val9\u001b[0m\n",
      "File config.yaml đã được tạo và ghi thành công.\n",
      "Ultralytics 8.3.9 🚀 Python-3.12.4 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 10971MiB)\n",
      "Model summary (fused): 168 layers, 3,006,818 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/robot/Desktop/pcb_temp/output/HQSR_S/labels... 1068 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1068/1068 [00:00<00:00, 3235.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/robot/Desktop/pcb_temp/output/HQSR_S/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1068/1068 [00:05<00:00, 195.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1068       2158      0.978      0.924      0.969      0.662\n",
      "            mouse_bite        168        332      0.983      0.919      0.973      0.664\n",
      "                  spur        169        348      0.926      0.911       0.95      0.638\n",
      "          missing_hole        190        379      0.994      0.995      0.994      0.713\n",
      "                 short        184        366      0.988      0.945      0.969      0.663\n",
      "          open_circuit        166        345      0.997      0.943      0.988      0.659\n",
      "       spurious_copper        191        388      0.979       0.83       0.94      0.635\n",
      "Speed: 0.5ms preprocess, 2.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "    # Nội dung của file YAML\n",
    "    # sub = 'VDSR'\n",
    "\n",
    "# for sub in ['HQSR_C', 'HQSR_S', 'E2DSR_C', 'E2DSR_S','EDSR', 'SRResNet', 'VDSR', 'SRCNN']:\n",
    "for sub in [ 'HQSR_C', 'HQSR_S']:\n",
    "\n",
    "    data = {\n",
    "        'train': f'dataset/pcb/pcb_test/images',\n",
    "        'val': f'./{sub}/images',\n",
    "        'nc': 6,\n",
    "        'names': {\n",
    "            0: 'mouse_bite',\n",
    "            1: 'spur',\n",
    "            2: 'missing_hole',\n",
    "            3: 'short',\n",
    "            4: 'open_circuit',\n",
    "            5: 'spurious_copper'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Tạo và ghi file YAML\n",
    "    with open('output/data.yaml', 'w') as file:\n",
    "        yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "    print(\"File config.yaml đã được tạo và ghi thành công.\")\n",
    "\n",
    "\n",
    "    import ultralytics\n",
    "    from ultralytics import YOLO\n",
    "\n",
    "    # Load a model\n",
    "    # model = YOLO(\"yolov8n.pt\")  # load an official model\n",
    "    model = YOLO(\"bestweight_2006.pt\")  # load a custom model\n",
    "\n",
    "    # Validate the model\n",
    "    metrics = model.val(data = 'output/data.yaml', batch = 1)  # no arguments needed, dataset and settings remembered\n",
    "    metrics.box.map  # map50-95\n",
    "    metrics.box.map50  # map50\n",
    "    metrics.box.map75  # map75\n",
    "    metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
